\chapter{EVENT LOCALIZATION}
 \label{chap:eventLo}
\section{Introduction}
Video event localization has been extensively studied with the objective of locating the action of interest, but locating an event in a video typically suffers from enormous combinatorial complexity.  Generally, output of such a task encompasses the 'action of interest' within a sub-volume.  The task can be more difficult than just event recognition alone, because localized actions are present only on a fraction of the frames in a video.  Some of the common strategies that have been tried on this kind of problem are efficient sub-window search ~\citep{subwindowsearch}, `selective search' strategy ~\citep{selectivesearch}  and ~\cite{tubelet}, but these either perform exhaustive search or perform iterative merging of the supervoxels. 

\par This chapter has been sectioned into four parts.  In Section \ref{sec:bst}, approaches for capturing the pixels that are in motion is discussed.  Techniques for extracting the salient regions in a given frame are explained in Section \ref{sec:sal}.  The craft for fusing the above two techniques is examined in Section \ref{sec:ts} and in Section \ref{sec:trac}, the way for obtaining spatio-temporal volume is elaborated.

\section{Background Subtraction Techniques} 
 \label{sec:bst}
\par One simple approach ~\citep{Basharat08} for the event localization is to understand the motion relativity and the position of pixel changes. This is based on the inspiration that components corresponding to an event show a similar flow of pixels.  This approach works significantly well in surveillance data but fails for human event detection.  It was later realized that by eliminating the background (static parts) of the visual frame,  possible event locations can be retained.  Several background subtraction techniques are available in \cite{Piccardi04}, among these, some are applied on static images while others are on dynamic video.  In case of our application, we can blend both these approaches to generate more reliable event detectors.

\subsection{Frame Differencing (FD)}
A very common approach for performing moving object segmentation is is to use frame differencing, but this approach is very sensitive to small changes and yields lots of noise when the camera is in motion.  A simpler way to implement this technique is to compute the absolute difference of gray scale/intensity values across consecutive frames. Consider $P(p_x,p_y,t)$, the intensity of pixel $(p_x,p_y)$ at frame t, then pixel $(p_x,p_y)$ is considered foreground when absolute difference is above a threshold $T$,$$\vert P(p_x,p_y,t+1) - P(p_x,p_y,t)t\vert > T$$
\par The robustness of this method depends on the speed of foreground elements.  Faster movements may require higher thresholds to reduce the noise.   An alternate approach is to replace difference of a single previous frame  by an average of multiple previous frames. The results are still noisy and not reliable.

\subsection{Mixture of Gaussian (MOG)}
MOG is one of the well known methods of extracting foreground information.  \cite{kaew}] models a background by a mixture of N Gaussian distributions.  The mixture weights represent the proportions in which the colors are retained in the frame.  At any instance $c$, a particular pixel $(p_x,p_y)$ has color space represented as $V(p_{x},p_{y},c)$, and the history of pixels for $T$ frames are given by 
$$X_{1},..,X_{t} = {V(p_{x},p_{y},c)~:~1\le c \le T }$$
The history is modelled by multiple N Gaussian mixtures,
$$P(X_{t})=\Sigma_{i}^{N}w_{i,t}\eta(X_{t}|\mu_{i,t},\Sigma_{i,t})$$
\par where $w_{i,t}$, $\mu_{i,t}$ and $\Sigma_{i,t}$ are the weight, mean and covariance of the $i^{th}$ Gaussian in the mixture at time $t$ respectively, and where $\eta$ is the Gaussian density function.  The general principle behind this approach is that when a foreground object occludes the background object, the color distributions will begin to change resulting in either the creation of a new distribution or the increase in the variance of an existing distribution.  The foreground object that is in motion is expected to have a larger variance, as compared to background pixels, until the foreground stops moving.


\subsection{Eigen Subtraction (ES)}
Eigen based background subtraction was considered an elegant method compared to the methods discussed above.  A sample of N images of the videos are obtained, the mean background image $\nu_b$ is computed and all the images are mean normalized with respect to $X$.  Principal component analysis on the mean normalized images is performed, with the idea that the high-dimensional images are often described by correlated variables and only a few meaningful dimensions account for most of the information.  But performing PCA for large images is not straightforward. Consider 100 images of dimension $100 \times 100$ pixels, then the dimension the generated covariance matrix would be $10000 \times 10000$, i.e, roughly 0.8 GB (considering 64 bit float values). Solving this is not feasible, hence a trick from linear algebra is used for a $M  \times N$ matrix with $M>N$ can at most have N-1 non-zero eigenvalues ~\cite{Duda01}. Eigenvalue decomposition of $X^TX$ is carried out:
$$X^TX\nu_i=\lambda_i\nu_i$$
$$XX^T(X\nu_i)=\lambda_i(X\nu_i)$$
The orthonormal eigenvector of $XX^T$ is found by normalizing $X\nu_i$ to unit length.  Once the eigenvectors of $XX^T$  are computed, projection of the current frame $F$ on the eigenvectors with top eigenvalues are determined, and the reconstructed frame $F'$ is obtained by using the projection coefficients and the eigenvectors.  The difference $F-F'$ would correspond to the mask on the moving object.

\begin{figure}[htpb]
   \begin{center}
	    \includegraphics[width=0.75\textwidth]{snaps/bgsub/results.eps}     
     \caption {Background subtraction techniques on multiple video classes}
   \label{fig:bgsub}
   \end{center}
 \end{figure}
\par Results of the foreground mask obtained using FD, ES and MOG are shown in Figure \ref{fig:bgsub}. Even though MOG is most preferred, it provides sparse and a unclear boundary in most of the sample videos.  In the `dance' video, the FD yields extraneous pixels due to camera movement in between the two dancers, which are not evident in ES and MOG.  Also FD produces spurious pixels for the `pommello' video because of the sharp background.  All methods fail in the case of grainy videos, hence background subtraction alone cannot suffice for event localization.

\section{Saliency Detection}
\label{sec:sal}
Visual saliency is a unique perceptual aspect that makes some items in the world stand out from their neighbours and focus attention on it immediately.  As we had discussed earlier about the different techniques to extract the eminent pixels which are in motion, sometimes pixels which are stationary and quite distinct might also play a important role in the the activity recognition.  Visual saliency is considered in attention modelling.  Most of saliency estimations techniques follow the following five steps which can be visualized in Figure \ref{fig:salap},
\begin{itemize}
\item{\textbf{Step 1: Color equalization:} This is performed by removing the tail in the color histogram.  It helps to improve the contrast in the image.  In the implementation, color equalization is done on each channel separately so that all channels can be provided to the segmentation algorithm.}

\item{ \textbf{Step 2: SLIC-based segmentation:}  It is a simple linear iterative clustering (SLIC) algorithm that clusters pixels in the combined five-dimensional color and image plane space to efficiently generate compact, nearly uniform super-pixels.  Any SLIC based clustering algorithm takes two parameters number of super-pixels and compactness of each super-pixel.  According to \cite{slic}, SLIC is fastest and most memory efficient compared to  other segmentation techniques like graph-cut and quick shift segmentation techniques.  Segmentation reduces the number of computations in the subsequent stages.}

\item{ \textbf{Step 3: Extract segment properties:} Extracting features of the region that are used in computing the saliency of each region.  Some features that are considered in the implementation are color based features (LAB, RGB) and texture based features (GLCM, Texture Flow).}

\item{\textbf{Step 4: Compute~saliency:} Saliency estimation techniques can be broadly categorized into bottom-up, top-down and information maximization, based on algorithmic computation. Earliest saliency based attention modelling was proposed by \cite{itti}.  It was inspired by the behaviour and the neuronal architecture of the early primate visual system.}

\item{\textbf{Step 5: Saliency cut:} Saliency maps are generally thresholded to obtain the salient mask.  In the implementation, grab-cut\citep{grabCut} performs segmentation by modelling foreground and background based on saliency estimation to obtain the mask where higher saliency  estimates correspond to the foreground while lower saliency estimate correspond to background.  Once the saliency masks are obtained, a few morphological operations are applied to remove the salt and pepper noise.}
\end{itemize}

\begin{figure}[htpb]
   \begin{center}
	    \includegraphics[width=0.95\textwidth]{snaps/sal/saliency.eps}     
     \caption {A general approach for saliency estimation}
   \label{fig:salap}
   \end{center}
 \end{figure}

\par Different saliency estimation techniques that were studied are discussed below.  All these techniques can be considered either at region level or pixel level, based on whether the segmentation of the image is considered or not.

\subsection{Hierarchical Color Based approach (HCB)} 
HCB is a top-down approach for computing saliency where saliency map of improbable pixels/regions are set to zero at every iteration.  Saliency of pixel/region $I_{x}$ is computed as follows,
$$ Iavg_{x} = \Sigma_{y} I_{y}~\exp^{-\frac{\parallel~p_{x} - p_{y}~\parallel}{w}}  $$
$$ saliency(I_{x}) =~\parallel I_{x} - Iavg_{x} \parallel $$
where $p_{x}$ is the position of the pixel $I_{x}$.  At each iteration, $w$ is decreased implying closer neighbourhood.  After each iteration, pixels/regions with $ saliency(I_{x})$ lesser than $0.1$ after normalization are eliminated.  The foreground pixels/regions are obtained by discarding the background pixels/regions.  The primary focus is to measure the variation from the average pixel intensity.

\subsection{Context Aware Based approach (CAB)}
In CAB, saliency estimates are averaged out at a different context windows.  The expressions for the estimation of saliency is given below,
$$ AllPairDistance(I_{x},I_{y},k) = \exp^{-\frac{\parallel~p_x - p_y~\parallel}{dw_k}}~\exp^{-\frac{\parallel~I_{x} - I_{y}~\parallel}{cw}}$$
$$ NormDistance(I_{y},k) = \Sigma_{x} AllPairDistance(I_{x},I_{y},k)$$
$$ saliency(I_{x}) = 1- \Sigma^{K}_{k=1}~dw_{k}~\Sigma_{y}~\frac{AllPairDistance(I_{x},I_{y},k)}{NormDistance(I_{y},k)}$$
where $cw$ and $K$ are color weight and number of context windows respectively.  Saliency measures the similarity with its neighbourhood weighted on proximity of the pixels/regions. $AllPairDistance(I_{x},I_{y},k)$ is a disparity measure across pairs of pixels/regions $(I_{x},I_{y})$ for different values of $dw_{k}$ while $NormDistance(I_{y},k)$ corresponds to the normalizing factor. This technique performs fairly well in discriminating between salient objects which are not in same scale.

\subsection{Spectral Distribution Based approach (SDB)} 
SDB is based on the intuition that pixels with small color distribution variances have high saliency values.  The extent of color distribution is measured from the spatial variance of all colors in the image.  \cite{spectralSal} proposed a technique to compute color spatial variances using Gaussian Mixture Models (GMMs).  All pixels/regions in the image are represented by GMMs using expectation maximization (EM) algorithm, where $w_{c}$, $\mu_{c}$ and $\Sigma_{c}$ are the weight, the mean color, and the covariance matrix of the $c^{th}$ mixture respectively. Each pixel/region is assigned to a mixture with the probability:
$$p(c | I_{x}) = \frac{w_{c}\eta(I_{x}| \mu_{c},\Sigma_{c})}{\Sigma_{c}w_{c}\eta(I_{x}| \mu_{c},\Sigma_{c}} $$
Suppose $x_{h}$ is the x coordinate (horizontal coordinate) of the pixel x. The spatial variance for x-dimension of color component c is computed as:
$$\sigma_{h}^{2}(c) = \frac{1}{|P|_{c}}\Sigma_{x}	p(c | I_{x}) \parallel x_{h} -M_{h}(c) \parallel^{2}$$
where $M_{h}(c) = \frac{1}{|P|_{c}}\Sigma_{x}p(c|I_{x})~x_h$ , and $|P|_{c} = \Sigma_{x}p(c | I_{x})$ is a normalization factor.  The vertical variance $\sigma_{v}^{2}(c)$ is defined similarly.  The spatial variance of a component c is combined as: $\sigma^{2}(c) = \sigma_{h}^{2}(c) + \sigma_{v}^{2}(c)$ which is then normalized.
The saliency $saliency(I_{x})$ of a specific pixel $I_{x}$ regarding to color spatial distribution is defined as the weighted sum:
$$saliency(I_{x}) = \Sigma_{c}p(c | I_{x})(1-\sigma^{2}(c))$$
This method yields very good results for many examples but the configurations requires tweaks for better results.

\subsection{Regional Contrast (RC)}
RC computes saliency of an image region by measuring contrast with respect to the entire image. Several approaches  \citep{globContrast}, {patchRarities}, \citep{salFilters} for computing saliency based on global contrast are known in the literature.  In this work, different techniques are fused and experimented.  It considers two contrast measures:
\subsubsection{Uniqueness}
Element uniqueness is contemplated with the assumption that image regions that stand out from other regions in certain aspects catch our attention and hence must be labelled as more salient.  Element uniqueness for segment $I_{x}$, given its position $p_{I_{x}}$ compared to all other $I_{y}$, is defined as follows:
$$uniqueness(I_{x}) = \Sigma_{y} \parallel~I_{x} -I_{y}~\parallel~\exp^{-\frac{\parallel~p_{x} - p_{y}~\parallel}{dw}}$$
where $dw$ controls the proximity of the neighbourhood, a higher value corresponds to wider neighbourhood.
\subsubsection{Distribution}
In general, colors belonging to the background will be distributed over the entire image exhibiting a high spatial variance, whereas foreground objects are generally more compact.  Element distribution measure for a segment $I_{x}$ using the spatial variance of its color is as follows:
$$distribution(I_{x}) = \Sigma_{y} \parallel~p_{y} -\mu_{x}~\parallel~\exp^{-\frac{\parallel~I_{x} - I_{y}~\parallel}{cw}}$$ 
where $\mu_{x} = \Sigma_{y}~p_{y}~exp^{-\frac{\parallel~I_{x} - I_{y}~\parallel}{cw}}$, measures the weighted mean position of the color of $I_{x}$ component.  Low variance indicates a spatially compact and object hence they are considered more salient than spatially widely distributed elements.

\par The above two contrast measures defined at the element level can be consolidated by the following expression:
$$saliency(I_{x}) = uniqueness(I_{x}) \exp^{-k~distribution(I_{x})}$$
where $k$ controls the emphasis on distribution while both the contrast measures are normalized. It is observed that the uniqueness function performs well in the case of single object saliency while for multiple object saliency, uniqueness together with distribution yields better results.  Saliency mask obtained using HCB, CAB, SDB and RC for some standard weizmann segmentation dataset images are shown in Figure \ref{fig:sal}.
\par It is evident from the Figure \ref{fig:sal} that HCB is very sensitive to the weights, CAB yields lot of false positives, SDB is quite random as it depends on the random seed used in GMM, and RC is very robust for most of the examples but certainly depends on the weights.

\begin{figure}[htpb]
   \begin{center}
	    \includegraphics[width=0.75\textwidth]{snaps/sal/saliencyall.eps}     
     \caption {Saliency mask obtained using multiple techniques}
   \label{fig:sal}
   \end{center}
 \end{figure}
 \section{Temporal Smoothening}
  \label{sec:ts}
Smoothening of the event masks obtained at the frame level is essential because of occlusion and illumination variations.  A common approach followed for the smoothening of frame was to blend the background subtraction measure and the saliency measure to obtain a \textit{visual attention score}. Higher visual attention scores ($\geqslant0.8$) were considered \textit{true foregrounds} while lower visual attention scores ($\leqslant0.2$)  were considered \textit{true background}.  Subsequently, models are built for prediction of other regions/pixels.  Three approaches for smoothening of the visual attention measure are described below and in all the approaches context before and after the frame are considered.

\subsection{Eigen Based model}
Eigen Based method is based on projecting the pixels along the direction of maximum variance for both the foreground and background.  Any pixel is assigned as belonging to the foreground if the projection along the direction of maximum variance is large along the foreground as compared to the background.  The directions of maximum variance are computed by gathering features (pixel level features like color, texture, gradient and position) of foreground/background pixels in the context and perform PCA.  This approach of smoothening did not yield good localization.

\subsection{Semi-supervised model}
Semi-supervised learning models are techniques where some of the samples in the training data are not labelled, and thus this appropriately fits the smoothening task.  In such techniques, the unlabelled data are made use of for capturing the manifold of the underlying data distribution, and to thus generalize better for new examples. 
\par In this work, label propagation is used, and it constructs a similarity graph over all pixels in a given context, where the nodes label are spread based on the similarity.   The labeled data act as sources that push out labels through unlabeled data.  This technique was computationally intensive as we are required to build a model using all pixels in the frame for given context.  There was no adaptation mechanism available, for faster convergence of the learning algorithm.

\subsection{Gaussian Mixture model}
Use of Gaussian mixture models proved to be most successful amongst the techniques considered. In this approach, two Gaussian mixture models were built using true foreground and background examples. Each and every pixels is assigned to either the foreground or background based on the likelihood scores of the foreground and background models.  One of the biggest advantages of this approach is its ability to adapt, i.e, models of previous frames can be fine-tuned for the current frames, which helps in training individual context models quickly.

\par Because of the swiftness and the robustness, the Gaussian mixture models were used for all the experiments.  Few post processing operation were carried over the predicted masks like median smoothing, region filling were performed to remove the salt and pepper noises and spurious regions that are small.  Figure \ref{fig:smoothen} shows smoothed mask obtained from the visual attention score.  It shows how the model captures interesting regions even if their visual attention scores are not so eminent in that frame.
\begin{figure}[!htpb]
   \begin{center}
	    \includegraphics[width=0.73\textwidth]{snaps/smooth/smoothening.eps}     
     \caption {Gaussian mixture based smoothening}
   \label{fig:smoothen}
   \medskip 
 	\small Odd rows depict the visual attention score, with red and green representing \textit{true foreground} and \textit{true background} respectively.  Even rows illustrate the smoothened mask obtained from the \textit{visual attention score} using the Gaussian mixture model. 
   \end{center}
 \end{figure}
 
\section{Tracking}
 \label{sec:trac}
Tracking focuses on drawing a bounding box around a region of interest and tracking them in a train of frames.  After detecting interesting regions and post-processing operations, the filtered masks are grouped into connected regions (blobs) and labelled by using a connected component labelling algorithm.  The bounding boxes of these blobs corresponding to a object/subject are computed by a novel approach.  It ensures that a mask is present even if there are any uneventful (no mask) or extremely eventful (too many mask) frames.
\par This task is accomplished by building two GMM models, one to characterize the location of the event (like x-y coordinates and size) and other to characterize region color properties (like color histogram).  The GMM model built with context of 4-8 frames gives the location and size of localized region in the frame.  Back projection \citep{backProj} of region of interest (obtained from mean of GMM model) on the given frame is measured. It determines the probability of a pixel belong to the object.  The back-projected image  will have region of interest in white, while it will be black for others.  A fine-tuning of the result is attained using the mean shift algorithm.  The intuition behind mean-shift algorithm is simple, the tracking window is shifted until it reaches the area of maximum pixel density.  When the object moves, obviously the movement is reflected in histogram of the back-projected image. 
\par In this manner, tracking an event without the knowledge of the location of interest is made possible.  The bounding boxes obtained are of different sizes, so all windows are extended to the shape of the largest enclosing one.  This train of  windows form the spatio-temporal volume (STV). Figure \ref{fig:tracking} depicts STV for multiple events : horse riding, javelin, horse racing and bench press videos.
\begin{figure}[htpb]
   \begin{center}
	    \includegraphics[width=0.75\textwidth]{snaps/track/tracking.eps}     
     \caption {Event tracking after obtaining the smoothened mask}
   \label{fig:tracking}
   \end{center}
 \end{figure}
\section{Summary}
In this chapter we proposed a novel approach to obtain the localized STV.  The approach is unsupervised, as no information about the exact location of the window is considered during training.  The STV obtained clearly highlights that background subtraction along with saliency are essential for localizing the event regions. In the next chapter study on different methods for obtaining the localized train of frames will be dealt.  These localized train of frames are then given to CNN for the prediction.