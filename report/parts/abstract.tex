\abstract

\noindent KEYWORDS: \hspace*{0.5em} \parbox[t]{4.4in}{Convolutional Neural Network; Spatio Temporal Volume; Background Subtraction; Saliency Estimation; Temporal Smoothening}

\vspace*{24pt}
%In last few years because of the digitization there have been breakthrough on volume of videos accessible, but all become worthless if they are not refined into some application.
The Internet has made huge volumes of videos accessible.  As many videos are quite long in duration, a summary obtained in terms of events will be useful.
One such application is that of a real time event recognition which has been a fundamental research area in the vision domain. It aims to recognize the actions and goals of multiple subjects from a given sequence of frames. The major challenges with real time event recognition are geometric and photometric variances, clutter background and complex camera motion. Most of the current solutions are based on extracting complex hand-crafted features, but these features do not work in all scenarios and and involves tedious and time consuming processes. 

\par Hence, the problem has been split into event localization and event identification tasks, by isolating all possible event regions in a given sequence of frames, followed by prediction. 

\par Just like social localization where social network users are connected based on the interest and hobbies of an individual, video event localization intends to connect pixels or regions showing similar color, texture and motion characteristics.  This was accomplished using an endemic approach to obtain visual attention score (VAS) by fusing background subtraction (procure pixels that alter in subsequent frames) and saliency estimation (derive objects in a frame). The background subtraction extracts pixels that are in motion while saliency  derives important regions based on color and texture attributes. Also, a novel method was proposed for temporal smoothening of the visual attention score  with the motivation to obtain similar regions in subsequent frames. This is important because VAS may be vulnerable to sudden camera motion and illumination variations.  Window tracking is applied over the temporal smoothening to obtain a bounding box that corresponds to an event. These bounding boxes are termed as spatio temporal volumes (STVs). The STVs extracted from video segments are supplied to a CNN to identify the corresponding event labels.

\par As part of event identification, an indigenous deep neural network toolkit was built using external libraries. Though the toolkit consists of a number of different techniques, only convolutional neural network (CNN) was exploited as it is acclaimed to be good for image classification. 
%Also investigation were done on providing  processed features as input along with the raw images to CNN.

\par Highlights of this approach include state-of-the-art results for a standard UCF-50 video dataset and an indigenous and innovative method for solving various problems like surveillance, video summarization and more.  
\pagebreak