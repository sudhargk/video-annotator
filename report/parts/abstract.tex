\abstract

\noindent KEYWORDS: \hspace*{0.5em} \parbox[t]{4.4in}{Convolutional Neural Network; Spatio Temporal Volumne; Background Subtraction; Saliency Estimation; Temporal Smoothening}

\vspace*{24pt}
In last few years because of the digitization there have been breakthrough on volume of videos accessible, but all become worthless if they are not refined into some application. One such application is a real time event recognition which has been a fundamental research arena in the vision domain. It aims to recognize the actions and goals of multiple subjects from a given sequence of frames. The major challenges with real time event recognition are geometric and photometric variances, clutter background and complex camera motion. Most of the current solutions are based on extracting complex handcrafted features, but these features does not work in all scenarios and is a tedious process. 

\par Hence a problem has been set by dimidiating it into event localization and event recognition task by isolation of all possible event regions in given train of frames and then predicting. 

\par Just like social localization that where social network users are connected based on interest and hobbies of a individual, video event localization intends to connect pixels or regions showing similar color, texture and motion characterstics.  This was accomplished by an endemic approach to obtain visual attention score (VAS) by fusing background subtraction (procure pixels that alter in subsequent frames) and saliency estimation (derive objects in frame) . The background subtraction extract pixels that are inmotion while saliency  derive important regions based on color and texture attributes. Also a  novel method was proposed for temporal smoothening of the visual attention score  with the motivation to obtain a similar region in subsequent frames. This is important because VAS may be vulnerable to sudden cammera motion and illumination variances.  Window tracking is supplemented over the temporal smoothening to obtain a bounding box that correspond to an event. These bounding boxes are termed as spatio temporal volume (STV). The STV extracted from video segments are supplied to a CNN to identify the corresponding event label.

\par As part of event recognition an indigenous deep neural network toolkit was built using external libraries. Though the toolkit encompass of different techniques, only convolutional neural network (CNN) was exploited as it is acclaimed for image classification. Also investigation were done on providing  processed features as input along with the raw images to CNN.

\par Highlights of this approach include state of the art results for standard UCF-50 video dataset and an indigenous and innovative method for solving various problems like survellance, video summarization and more.  
\pagebreak