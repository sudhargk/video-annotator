\appendix
\label{app:pydnn}
\chapter{How to Use : Python-DNN}
Python-dnn uses python (and theano) to implement major Deep Learning Networks. The toolkit currently supports neural network model CNN, SDA, DBN and more. It also encompass a  general DNN  kit with support of multiple activation functions. Python-dnn functions as toolkit as well as library allowing to extend and build wrappers over it.
\section{Installation}
Installing  the python-dnn as library can be easily done using the pip utility.
\begin{lstlisting}[language=bash,basicstyle=\small] 
sudo pip install https://github.com/IITM-DONLAB/python-dnn/zipball/master
\end{lstlisting}
Python-dnn requires 
\begin{itemize}
	\item Python ($\geq$ 2.6 , $<$ 3.0),
	\item NumPy ($\geq$ 1.6.1),
	\item Theano ($\geq$ 0.7)
	\item matplotlib ($\geq$ 1.4.3).
\end{itemize}
\noindent Latest development version of the stand-alone toolkit is available at:
\begin{lstlisting}[language=bash,basicstyle=\small] 
git clone git://github.com/IITM-DONLAB/python-dnn.git
\end{lstlisting}

\section{Toolkit Configuration} 
\subsubsection{Model Configuration (model.conf)}
\begin{table}[!htbp]
   \begin{center}
   \begin{tabular}{|l|p{10cm}|l|} \hline
   	\textbf{parameter} & \textbf{description} & \textbf{default}\\  \hline
     *nnetType & type of Network (CNN/RBM/SDA/DNN) & \\
     *train\_data & working directory containing data configuration and output \\ \hline
	 *wdir & working directory & \\ \hline
 	 *data\_spec & path for data specification relative to model.conf & \\ \hline
	 *nnet\_spec & path for network configuration specification relative to model.conf & \\ \hline
	 *output\_file & path for network model file relative to wdir & \\ \hline
	 input\_file & path for pre-trained/fine-tuned model relative to wdir. & \\ \hline
	 random\_seed & random seed for initialization of weights & none \\ \hline
	 logger\_level & logger level : 'INFO','DEBUG' and 'ERROR' & 'INFO' \\ \hline
	 batch\_size & mini batch size  & 128 \\ \hline
	 *n\_ins & input dimension  & \\ \hline
	 *n\_outs & output dimension (num classes) & \\ \hline
	 finetune & 
	 	\begin{tabular}{r|p{6cm}} %\hlne
	 	 momentum & momentum factor for finetuning \\
	 	 method & C : constant learning rate  E :exponential decay (default :C) \\
 	   \end{tabular}	
	 &  \\ \hline
	 export\_path &  path for writing (bottleneck) features relative to model.conf & \\ \hline 	 
	 processes & 
	 	\begin{tabular}{r|p{6cm}} %\hlne
	 	 pretraining & to do pre-training or not \\
	 	 finetuning & to do fine-tuning or not \\
	 	 testing & to do testing or not \\
	 	 export\_data & to do feat extraction or not. If true export\_path is mandatory \\
 	   \end{tabular}	
	 & false \\ \hline
	 save\_freq & epoch interval for saving model & \\ \hline 	 	
   \end{tabular}		
   \end{center}
 \end{table} 
\clearpage

\subsubsection{Data Configuration (data.conf)}
\begin{table}[!htbp]
\begin{center}
  \medskip  \small configuration for training/testing/validation.
   \begin{tabular}{|l|p{8cm}|c|} \hline
   	\textbf{parameter} & \textbf{description} & \textbf{default}\\  \hline
 	*base\_path & Base path of data. &  \\  \hline
   	*filename &  train/test/val filename & \\  \hline
	*partition & data size (in MiB) to be loaded in memory & \\  \hline
	random & allow random ordering  & true \\  \hline
	random\_seed & seed for random numbers if random is true & \\  \hline 
	keep\_flatten & flatten vector or reshape to input\_shape & false \\  \hline
	*reader\_type & reader type : NP/T1/T2. & \\  \hline		
	*input\_shape & shape of input data & \\  \hline
	*dim\_shuffle &  shuffle order of the input data &  \\ \hline
  \end{tabular}		
\end{center}
\end{table} 
\noindent For the purpose of using the toolkit, data have to be in one of the file format.
\begin{itemize}
\item{\textbf {Numpy Format [NP]:}The dataset is stored as single file in binary format in following structure,
\begin{lstlisting}[language=bash,basicstyle=\small] 
<json-header>
<structured numpy.array>
<structured numpy.array>
..
\end{lstlisting}
json-header  : featdim (dimension of input vector after flattening), input\_shape (shape of input).}

\item{\textbf {Text File (One level header) [T1]:} The dataset contains a root file with list of  text file names corresponding to a class. It has following format,
\begin{lstlisting}[language=bash,basicstyle=\small] 
<feat_dim> <num_classes>
<data_file1>
<data_file2>
..
\end{lstlisting}
data\_file correspond to individual class files with following structure,
\begin{lstlisting}[basicstyle=\small] 
<feat_dim> <num_feat_vectors(optional)>
<feat_vector>
<feat_vector>
..
\end{lstlisting}}

\item{\textbf {Text File (Two level header) [T2]:} This format has got extra level of indirection, the root file has got following structure,
\begin{lstlisting}[language=bash,basicstyle=\small] 
<feat_dim> <num_classes>
<class_index_file1>
<class_index_file2>
..
\end{lstlisting}
class\_index\_file constitute of list of data filenames belonging to single class, 
\begin{lstlisting}[basicstyle=\small] 
<data_file1>
<data_file2>
..
\end{lstlisting}}
\end{itemize}

\subsubsection{Network Configuration (nnet.conf)}
\begin{table}[!htbp]
\begin{center}
  \medskip  \small configuration of cnn
   \begin{tabular}{|c|p{12cm}|} \hline
   	\textbf{parameter} & \textbf{description} \\  \hline
   	 cnn & 
	 \begin{tabular}{c|p{9cm}} %\hlne
	 layers+ & 
		\begin{tabular}{r|p{6cm}} %\hlne
		convmat\_dim & dimension of convolution weights \\  \hline
		num\_filters & number of feature maps \\  \hline
		poolsize & max-pooling dimensions \\  \hline
		update & updated weight during training. \\  \hline
		activation & activation function used by the layer \\ 
		\end{tabular} \\ \hline
	  activation & global activation function \\ \hline
	  use\_fast & use pylearn2 library for faster computation \\ 
 	   \end{tabular}	 \\ \hline
 	 mlp & 
	 \begin{tabular}{c|p{9cm}} %\hlne 
	  layers &  hidden layer sizes \\ \hline
	  adv\_activation & 
		\begin{tabular}{r|p{6cm}} %\hlne
			method &  'maxout','pnorm' \\ \hline
			pool\_size & pool size (in pnorm) \\ \hline
			pnorm\_order & norm order for pnorm. (Default: 1) \\
		\end{tabular} \\ \hline
	  activation & activation function for mlp layers. (if adv\_activation is used, then either 'linear','relu' or 'cappedrelu') \\ 
 \end{tabular}	 \\ \hline
  \end{tabular}		
\end{center}
 \end{table} 

\begin{table}[!htbp] 
 \begin{center}
  	\medskip  \small configuration of dnn
   	\begin{tabular}{|c|p{8cm}|c|} \hline
   	\textbf{parameter} & \textbf{description} & \textbf{default}\\  \hline
	*hidden\_layers &  RBM layer sizes. & \\ \hline
	pretrained\_layers & number of layers  pre-trained. & 0 \\ \hline
	activation & activation function for the layers. (if adv\_activation is used, then either 'linear','relu' or 'cappedrelu') & tanh or linear \\ \hline
	max\_col\_norm & The max value of norm of gradients (in dropout and maxout)	& null \\ \hline
	l1\_reg &  l1 norm regularization & 0 \\ \hline
	l2\_reg &  l2 norm regularization & 0 \\ \hline
	adv\_activation & 
		\begin{tabular}{r|p{5cm}} %\hlne
		method &  'maxout','pnorm' \\ \hline
		pool\_size & pool size (in pnorm) \\ \hline
		pnorm\_order & norm order for pnorm. (Default: 1) \\
		\end{tabular} & \\ \hline
	do\_dropout &  use dropout or not & false  \\ \hline
	dropout\_factor & dropout factors for DNN layers. & [0.0] \\ \hline
	input\_dropout\_factor & dropout factor for input features & 0.0 \\ \hline
	\end{tabular}
\end{center}
\end{table} 
 
\begin{table}[!htbp] 
 \begin{center}
  	\medskip  \small configuration of dbn (rbm)
	\begin{tabular}{|c|p{8cm}|c|} \hline
   	\textbf{parameter} & \textbf{description} & \textbf{default}\\  \hline
	*hidden\_layers &  RBM layer sizes. & \\ \hline
	activation & activation function for the layers. & tanh \\ \hline
	pretrained\_layers & number of layers  pre-trained. & 0 \\ \hline
	first\_layer\_type & 'bb' (Bernoulli-Bernoulli) or 'gb' (Gaussian-Bernoulli) & gb  \\ 	\hline 
	\end{tabular}		
\end{center}
\end{table} 
\begin{table}[!htbp] 
 \begin{center}
  	\medskip  \small configuration of dbn (sda)
	\begin{tabular}{|c|p{8cm}|c|} \hline
   	\textbf{parameter} & \textbf{description} & \textbf{default}\\  \hline
	*hidden\_layers &  hidden denoising autoencoder layer sizes & \\ \hline
	activation & activation function for the layers & tanh \\ \hline
	*corruption\_levels & corruption level for each layer &  \\ \hline
	\end{tabular}		
\end{center} 
\end{table} 
\noindent It is quite evident in all model configuration the activation functions are crucial. The activation function are used to transform the activation level of a unit (neuron) into an output signal. Generally, activation functions have a "squashing" effect. Python-DNN currently support following activation functions,
\begin{itemize}
\item {\textbf{sigmoid:} sigmoid function with equation: $f(x) = \frac{1}{(1 + \exp^{-x}}$. This is an S-shaped (sigmoid) curve, with output in the range $(0,1)$.}
\item {\textbf{tanh:} hyperbolic tangent function is a sigmoid curve, like the logistic function, except that output lies in the range $(-1,+1)$.} 
\item {\textbf{relu:} rectifier linear unit is an activation function defined as $f(x) = max(0, x)$}
\item {\textbf{cappedrelu:} same as ReLU except we cap the units at 6.ie, $f(x) = min(max(x,0),6)$}
\end{itemize}
\clearpage

\section{Usage}
Quick steps for modelling any dataset using the toolkit,
\begin{itemize}
	\item{\textbf{Step 1 : Prepare Dataset:} The datasets must converted to one of the following data formats NP/T1/T2 formats. }
	\item{\textbf{Step 2 : Configure model:} A sample model configuration is copied from a standard dataset, depending on the type of deep neural network set the value of \textit{nnetType} and change \textit{wdir} to the result directory location. Update \textit{n\_ins} to input shape of neural network layer, for e.g 2d CNN accepting an image with dimension 80x60 with all 3 channel could be represented as [3,60,80],be careful in case of CNN's order plays crucial role. Set \textit{n\_outs} to number of classes for supervised learning task. Update the \textit{processes} flag based on type of model type like CNN do not have pre-training while for RBM and SDA pre-training is default.}
	\item {\textbf{Step 3 : Configure data:} A sample data configuration is copied to same directory of the model configuration file. Set the \textit{base\_path} and \textit{filename} to the directory of data and NP/T1/T2 filename respectively. The parameter \textit{partition} need to be set properly in case of large dataset, assign value (in MB) less than primary memory/gpu memory. Update the \textit{reader\_type} based on data format of the file. \textit{input\_shape} define the actual shape in which data was flattened while \textit{dim\_shuffle} allow to reorder the dimension as per the input to neural network.}
	\item{\textbf{Step 4 : Configure neural net:} A sample nnet configuration is copied to same directory of model configuration file. Do necessary changes as per the input shape of the data.}	
	\item{\textbf{Step 5 : Set environment flags:} For cpu mode, set \textit{device} to cpu, while for gpu mode, set \textit{device} to gpu. Set the following environment flag.
	\begin{lstlisting}[language=bash,basicstyle=\small] 
		THEANO_FLAGS=device=cpu,floatX=float32
	\end{lstlisting}} 
	
	\item{\textbf{Step 6 : Run toolkit:} The toolkit can be run by executing the \textit{python-dnn} script where \textit{model.conf} is the model configuration file.
	\begin{lstlisting}[language=bash,basicstyle=\small] 
		./python-dnn <model.conf>
	\end{lstlisting}	} 
\end{itemize}

